{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GenoSpark for Singapore population genotypic frequency\n",
    "\n",
    "## First, generate the output files with the command syntax below:\n",
    "```\n",
    "$ vcftools --gzvcf chr[#].consolidate.eff.PPH.vcf.gz --freq --chr [#] --out chr[#]_analysis\n",
    "$ bcftools query -f '%CHROM\\t%POS\\t%ID\\n' chr[#].consolidate.eff.PPH.vcf.gz -o chr[#]_rsID\n",
    "```\n",
    "\n",
    "#### * You may try freqGenerator.sh to generate the above files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3.5\n",
    "\n",
    "__author__ = 'mdc_hk'\n",
    "version = '1.0'\n",
    "\n",
    "# Description: To build the database on the pyspark DataFrame\n",
    "# Usage: -\n",
    "# Example: -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime, multiprocessing, os, re, shutil, sys, subprocess, time, logging\n",
    "\n",
    "import pyspark.sql.types as typ\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "\n",
    "# Specify schemas\n",
    "\n",
    "schema_Freq = typ.StructType([\n",
    "    typ.StructField(\"CHROM\", typ.IntegerType(), False),\n",
    "    typ.StructField(\"POS\", typ.IntegerType(), False),\n",
    "    typ.StructField(\"N_ALLELES\", typ.IntegerType(), False),\n",
    "    typ.StructField(\"N_CHR\", typ.IntegerType(), False),\n",
    "    typ.StructField(\"ALLELE_FREQ_1\", typ.StringType(), False),\n",
    "    typ.StructField(\"ALLELE_FREQ_2\", typ.StringType(), False),\n",
    "])\n",
    "\n",
    "schema_rsID = typ.StructType([\n",
    "    typ.StructField(\"CHROM\", typ.IntegerType(), False),\n",
    "    typ.StructField(\"POS\", typ.IntegerType(), False),\n",
    "    typ.StructField(\"ID\", typ.StringType(), True),\n",
    "])\n",
    "\n",
    "schema_Freq_DF = typ.StructType([\n",
    "    typ.StructField(\"CHROM\", typ.IntegerType(), False),\n",
    "    typ.StructField(\"POS\", typ.IntegerType(), False),\n",
    "    typ.StructField(\"N_ALLELES\", typ.IntegerType(), False),\n",
    "    typ.StructField(\"N_CHR\", typ.IntegerType(), False),\n",
    "    typ.StructField(\"ALLELE_FREQ_1\", typ.StringType(), False),\n",
    "    typ.StructField(\"ALLELE_FREQ_2\", typ.StringType(), False),\n",
    "    typ.StructField(\"ID\", typ.StringType(), True),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['chr11'], ['chr1'], ['chr10'], ['chr9'], ['chr2'], ['chr3'], ['chr4'], ['chr5'], ['chr6'], ['chr7'], ['chr8'], ['chr12'], ['chr13'], ['chr14'], ['chr15'], ['chr16'], ['chr17'], ['chr18'], ['chr19'], ['chr20'], ['chr21'], ['chr22']]\n"
     ]
    }
   ],
   "source": [
    "# Setting up File Paths and Lists\n",
    "\n",
    "workingFolder_Indian = os.getcwd() + \"/SgIndian_vcf/dataFreeze_Feb2013/SNP/biAllele/\"\n",
    "workingFolder_Malay = os.getcwd() + \"/SgMalay_vcf/2012_05/snps/\"\n",
    "\n",
    "# Filing number of unique samples found in the working folder...\n",
    "\n",
    "freqFiles_Indian = [f for f in os.listdir(workingFolder_Indian) if re.match(r'chr\\d+_analysis\\.frq', f)]\n",
    "rsIDFiles_Indian = [f for f in os.listdir(workingFolder_Indian) if re.match(r'chr\\d+_rsID', f)]\n",
    "freqFiles_Malay = [f for f in os.listdir(workingFolder_Malay) if re.match(r'chr\\d+_analysis\\.frq', f)]\n",
    "rsIDFiles_Malay = [f for f in os.listdir(workingFolder_Malay) if re.match(r'chr\\d+_rsID', f)]\n",
    "\n",
    "freqFilesID_pre = re.compile(r'(chr\\d+)_analysis\\.frq')\n",
    "freqFilesID = []\n",
    "for file in freqFiles_Indian:\n",
    "    freqFilesID.append(freqFilesID_pre.findall(file))\n",
    "\n",
    "print(freqFilesID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Obtain dataset\n",
    "\n",
    "# suffixFreqID = ['_analysis.frq', '_rsID']\n",
    "freqDF_Indian = spark.createDataFrame([], schema_Freq_DF)\n",
    "freqDF_Malay = spark.createDataFrame([], schema_Freq_DF)\n",
    "\n",
    "for ID in freqFilesID:\n",
    "    df1 = spark.read.csv(workingFolder_Indian + ID[0] + \"_analysis.frq\", header=True, schema=schema_Freq, sep='\\t').alias('df1')\n",
    "    df2 = spark.read.csv(workingFolder_Indian + ID[0] + \"_rsID\", header=False, schema=schema_rsID, sep='\\t').alias('df2')\n",
    "    freqChrN_working = df2.join(df1, df2.POS == df1.POS).select('df1.*','df2.ID')\n",
    "    freqDF_Indian = freqDF_Indian.union(freqChrN_working)\n",
    "    \n",
    "for ID in freqFilesID:\n",
    "    df1 = spark.read.csv(workingFolder_Malay + ID[0] + \"_analysis.frq\", header=True, schema=schema_Freq, sep='\\t').alias('df1')\n",
    "    df2 = spark.read.csv(workingFolder_Malay + ID[0] + \"_rsID\", header=False, schema=schema_rsID, sep='\\t').alias('df2')\n",
    "    freqChrN_working = df2.join(df1, df2.POS == df1.POS).select('df1.*','df2.ID')\n",
    "    freqDF_Malay = freqDF_Malay.union(freqChrN_working)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freqDF_Indian_working = freqDF_Indian.withColumn(\"ETHNIC\", lit(\"Indian\"))\n",
    "freqDF_Malay_working = freqDF_Malay.withColumn(\"ETHNIC\", lit(\"Malay\"))\n",
    "freqDF_working = freqDF_Malay_working.union(freqDF_Indian_working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+-----+-------------+-------------+-----------+------+\n",
      "|CHROM|   POS|N_ALLELES|N_CHR|ALLELE_FREQ_1|ALLELE_FREQ_2|         ID|ETHNIC|\n",
      "+-----+------+---------+-----+-------------+-------------+-----------+------+\n",
      "|   11|103739|        2|   30|   T:0.933333|  C:0.0666667|          .|Indian|\n",
      "|   11|104469|        2|   14|   G:0.714286|   A:0.285714| SSPM_MATCH|Indian|\n",
      "|   11|105023|        2|   42|   G:0.928571|  A:0.0714286|          .|Indian|\n",
      "|   11|105073|        2|   46|   G:0.956522|  A:0.0434783|          .|Indian|\n",
      "|   11|111159|        2|   68|   T:0.897059|   C:0.102941|          .|Indian|\n",
      "|   11|124986|        2|   24|   G:0.583333|   A:0.416667| SSPM_MATCH|Indian|\n",
      "|   11|150695|        2|   42|   C:0.880952|   T:0.119048| SSPM_MATCH|Indian|\n",
      "|   11|153453|        2|   46|   A:0.152174|   G:0.847826|rs187516525|Indian|\n",
      "|   11|158635|        2|   46|  C:0.0434783|   T:0.956522|  rs4109479|Indian|\n",
      "|   11|191770|        2|   72|   C:0.930556|  T:0.0694444| SSPM_MATCH|Indian|\n",
      "+-----+------+---------+-----+-------------+-------------+-----------+------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+------+---------+-----+-------------+-------------+\n",
      "|ETHNIC|       ID|CHROM|ALLELE_FREQ_1|ALLELE_FREQ_2|\n",
      "+------+---------+-----+-------------+-------------+\n",
      "|Indian|rs4109479|   11|  C:0.0434783|   T:0.956522|\n",
      "+------+---------+-----+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "freqDF_working.show(10)\n",
    "freqDF_working.select(\"ETHNIC\", \"ID\", \"CHROM\", \"ALLELE_FREQ_1\", \"ALLELE_FREQ_2\").filter(\"ID == 'rs4109479'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# spark.sql(\"select * from freqDF_working\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1681.count.\n: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange SinglePartition\n+- *HashAggregate(keys=[], functions=[partial_count(1)], output=[count#3210L])\n   +- Union\n      :- *Project\n      :  +- Scan ExistingRDD[CHROM#15,POS#16,N_ALLELES#17,N_CHR#18,ALLELE_FREQ_1#19,ALLELE_FREQ_2#20,ID#21]\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#1563], [POS#1549], Inner, BuildRight\n      :     :- *Project [POS#1563]\n      :     :  +- *Filter isnotnull(POS#1563)\n      :     :     +- *FileScan csv [POS#1563] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#1549]\n      :           +- *Filter isnotnull(POS#1549)\n      :              +- *FileScan csv [POS#1549] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#1632], [POS#1618], Inner, BuildRight\n      :     :- *Project [POS#1632]\n      :     :  +- *Filter isnotnull(POS#1632)\n      :     :     +- *FileScan csv [POS#1632] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#1618]\n      :           +- *Filter isnotnull(POS#1618)\n      :              +- *FileScan csv [POS#1618] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#1701], [POS#1687], Inner, BuildRight\n      :     :- *Project [POS#1701]\n      :     :  +- *Filter isnotnull(POS#1701)\n      :     :     +- *FileScan csv [POS#1701] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#1687]\n      :           +- *Filter isnotnull(POS#1687)\n      :              +- *FileScan csv [POS#1687] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#1770], [POS#1756], Inner, BuildRight\n      :     :- *Project [POS#1770]\n      :     :  +- *Filter isnotnull(POS#1770)\n      :     :     +- *FileScan csv [POS#1770] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#1756]\n      :           +- *Filter isnotnull(POS#1756)\n      :              +- *FileScan csv [POS#1756] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#1839], [POS#1825], Inner, BuildRight\n      :     :- *Project [POS#1839]\n      :     :  +- *Filter isnotnull(POS#1839)\n      :     :     +- *FileScan csv [POS#1839] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#1825]\n      :           +- *Filter isnotnull(POS#1825)\n      :              +- *FileScan csv [POS#1825] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#1908], [POS#1894], Inner, BuildRight\n      :     :- *Project [POS#1908]\n      :     :  +- *Filter isnotnull(POS#1908)\n      :     :     +- *FileScan csv [POS#1908] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#1894]\n      :           +- *Filter isnotnull(POS#1894)\n      :              +- *FileScan csv [POS#1894] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#1977], [POS#1963], Inner, BuildRight\n      :     :- *Project [POS#1977]\n      :     :  +- *Filter isnotnull(POS#1977)\n      :     :     +- *FileScan csv [POS#1977] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#1963]\n      :           +- *Filter isnotnull(POS#1963)\n      :              +- *FileScan csv [POS#1963] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#2046], [POS#2032], Inner, BuildRight\n      :     :- *Project [POS#2046]\n      :     :  +- *Filter isnotnull(POS#2046)\n      :     :     +- *FileScan csv [POS#2046] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#2032]\n      :           +- *Filter isnotnull(POS#2032)\n      :              +- *FileScan csv [POS#2032] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#2115], [POS#2101], Inner, BuildRight\n      :     :- *Project [POS#2115]\n      :     :  +- *Filter isnotnull(POS#2115)\n      :     :     +- *FileScan csv [POS#2115] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#2101]\n      :           +- *Filter isnotnull(POS#2101)\n      :              +- *FileScan csv [POS#2101] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#2184], [POS#2170], Inner, BuildRight\n      :     :- *Project [POS#2184]\n      :     :  +- *Filter isnotnull(POS#2184)\n      :     :     +- *FileScan csv [POS#2184] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#2170]\n      :           +- *Filter isnotnull(POS#2170)\n      :              +- *FileScan csv [POS#2170] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#2253], [POS#2239], Inner, BuildRight\n      :     :- *Project [POS#2253]\n      :     :  +- *Filter isnotnull(POS#2253)\n      :     :     +- *FileScan csv [POS#2253] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#2239]\n      :           +- *Filter isnotnull(POS#2239)\n      :              +- *FileScan csv [POS#2239] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#2322], [POS#2308], Inner, BuildRight\n      :     :- *Project [POS#2322]\n      :     :  +- *Filter isnotnull(POS#2322)\n      :     :     +- *FileScan csv [POS#2322] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#2308]\n      :           +- *Filter isnotnull(POS#2308)\n      :              +- *FileScan csv [POS#2308] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#2391], [POS#2377], Inner, BuildRight\n      :     :- *Project [POS#2391]\n      :     :  +- *Filter isnotnull(POS#2391)\n      :     :     +- *FileScan csv [POS#2391] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#2377]\n      :           +- *Filter isnotnull(POS#2377)\n      :              +- *FileScan csv [POS#2377] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#2460], [POS#2446], Inner, BuildRight\n      :     :- *Project [POS#2460]\n      :     :  +- *Filter isnotnull(POS#2460)\n      :     :     +- *FileScan csv [POS#2460] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#2446]\n      :           +- *Filter isnotnull(POS#2446)\n      :              +- *FileScan csv [POS#2446] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#2529], [POS#2515], Inner, BuildRight\n      :     :- *Project [POS#2529]\n      :     :  +- *Filter isnotnull(POS#2529)\n      :     :     +- *FileScan csv [POS#2529] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#2515]\n      :           +- *Filter isnotnull(POS#2515)\n      :              +- *FileScan csv [POS#2515] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#2598], [POS#2584], Inner, BuildRight\n      :     :- *Project [POS#2598]\n      :     :  +- *Filter isnotnull(POS#2598)\n      :     :     +- *FileScan csv [POS#2598] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#2584]\n      :           +- *Filter isnotnull(POS#2584)\n      :              +- *FileScan csv [POS#2584] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#2667], [POS#2653], Inner, BuildRight\n      :     :- *Project [POS#2667]\n      :     :  +- *Filter isnotnull(POS#2667)\n      :     :     +- *FileScan csv [POS#2667] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#2653]\n      :           +- *Filter isnotnull(POS#2653)\n      :              +- *FileScan csv [POS#2653] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#2736], [POS#2722], Inner, BuildRight\n      :     :- *Project [POS#2736]\n      :     :  +- *Filter isnotnull(POS#2736)\n      :     :     +- *FileScan csv [POS#2736] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#2722]\n      :           +- *Filter isnotnull(POS#2722)\n      :              +- *FileScan csv [POS#2722] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#2805], [POS#2791], Inner, BuildRight\n      :     :- *Project [POS#2805]\n      :     :  +- *Filter isnotnull(POS#2805)\n      :     :     +- *FileScan csv [POS#2805] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#2791]\n      :           +- *Filter isnotnull(POS#2791)\n      :              +- *FileScan csv [POS#2791] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#2874], [POS#2860], Inner, BuildRight\n      :     :- *Project [POS#2874]\n      :     :  +- *Filter isnotnull(POS#2874)\n      :     :     +- *FileScan csv [POS#2874] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#2860]\n      :           +- *Filter isnotnull(POS#2860)\n      :              +- *FileScan csv [POS#2860] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#2943], [POS#2929], Inner, BuildRight\n      :     :- *Project [POS#2943]\n      :     :  +- *Filter isnotnull(POS#2943)\n      :     :     +- *FileScan csv [POS#2943] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#2929]\n      :           +- *Filter isnotnull(POS#2929)\n      :              +- *FileScan csv [POS#2929] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#3012], [POS#2998], Inner, BuildRight\n      :     :- *Project [POS#3012]\n      :     :  +- *Filter isnotnull(POS#3012)\n      :     :     +- *FileScan csv [POS#3012] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#2998]\n      :           +- *Filter isnotnull(POS#2998)\n      :              +- *FileScan csv [POS#2998] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- Scan ExistingRDD[CHROM#0,POS#1,N_ALLELES#2,N_CHR#3,ALLELE_FREQ_1#4,ALLELE_FREQ_2#5,ID#6]\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#45], [POS#31], Inner, BuildRight\n      :     :- *Project [POS#45]\n      :     :  +- *Filter isnotnull(POS#45)\n      :     :     +- *FileScan csv [POS#45] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#31]\n      :           +- *Filter isnotnull(POS#31)\n      :              +- *FileScan csv [POS#31] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#114], [POS#100], Inner, BuildRight\n      :     :- *Project [POS#114]\n      :     :  +- *Filter isnotnull(POS#114)\n      :     :     +- *FileScan csv [POS#114] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#100]\n      :           +- *Filter isnotnull(POS#100)\n      :              +- *FileScan csv [POS#100] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#183], [POS#169], Inner, BuildRight\n      :     :- *Project [POS#183]\n      :     :  +- *Filter isnotnull(POS#183)\n      :     :     +- *FileScan csv [POS#183] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#169]\n      :           +- *Filter isnotnull(POS#169)\n      :              +- *FileScan csv [POS#169] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#252], [POS#238], Inner, BuildRight\n      :     :- *Project [POS#252]\n      :     :  +- *Filter isnotnull(POS#252)\n      :     :     +- *FileScan csv [POS#252] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#238]\n      :           +- *Filter isnotnull(POS#238)\n      :              +- *FileScan csv [POS#238] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#321], [POS#307], Inner, BuildRight\n      :     :- *Project [POS#321]\n      :     :  +- *Filter isnotnull(POS#321)\n      :     :     +- *FileScan csv [POS#321] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#307]\n      :           +- *Filter isnotnull(POS#307)\n      :              +- *FileScan csv [POS#307] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#390], [POS#376], Inner, BuildRight\n      :     :- *Project [POS#390]\n      :     :  +- *Filter isnotnull(POS#390)\n      :     :     +- *FileScan csv [POS#390] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#376]\n      :           +- *Filter isnotnull(POS#376)\n      :              +- *FileScan csv [POS#376] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#459], [POS#445], Inner, BuildRight\n      :     :- *Project [POS#459]\n      :     :  +- *Filter isnotnull(POS#459)\n      :     :     +- *FileScan csv [POS#459] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#445]\n      :           +- *Filter isnotnull(POS#445)\n      :              +- *FileScan csv [POS#445] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#528], [POS#514], Inner, BuildRight\n      :     :- *Project [POS#528]\n      :     :  +- *Filter isnotnull(POS#528)\n      :     :     +- *FileScan csv [POS#528] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#514]\n      :           +- *Filter isnotnull(POS#514)\n      :              +- *FileScan csv [POS#514] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#597], [POS#583], Inner, BuildRight\n      :     :- *Project [POS#597]\n      :     :  +- *Filter isnotnull(POS#597)\n      :     :     +- *FileScan csv [POS#597] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#583]\n      :           +- *Filter isnotnull(POS#583)\n      :              +- *FileScan csv [POS#583] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#666], [POS#652], Inner, BuildRight\n      :     :- *Project [POS#666]\n      :     :  +- *Filter isnotnull(POS#666)\n      :     :     +- *FileScan csv [POS#666] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#652]\n      :           +- *Filter isnotnull(POS#652)\n      :              +- *FileScan csv [POS#652] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#735], [POS#721], Inner, BuildRight\n      :     :- *Project [POS#735]\n      :     :  +- *Filter isnotnull(POS#735)\n      :     :     +- *FileScan csv [POS#735] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#721]\n      :           +- *Filter isnotnull(POS#721)\n      :              +- *FileScan csv [POS#721] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#804], [POS#790], Inner, BuildRight\n      :     :- *Project [POS#804]\n      :     :  +- *Filter isnotnull(POS#804)\n      :     :     +- *FileScan csv [POS#804] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#790]\n      :           +- *Filter isnotnull(POS#790)\n      :              +- *FileScan csv [POS#790] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#873], [POS#859], Inner, BuildRight\n      :     :- *Project [POS#873]\n      :     :  +- *Filter isnotnull(POS#873)\n      :     :     +- *FileScan csv [POS#873] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#859]\n      :           +- *Filter isnotnull(POS#859)\n      :              +- *FileScan csv [POS#859] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#942], [POS#928], Inner, BuildRight\n      :     :- *Project [POS#942]\n      :     :  +- *Filter isnotnull(POS#942)\n      :     :     +- *FileScan csv [POS#942] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#928]\n      :           +- *Filter isnotnull(POS#928)\n      :              +- *FileScan csv [POS#928] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#1011], [POS#997], Inner, BuildRight\n      :     :- *Project [POS#1011]\n      :     :  +- *Filter isnotnull(POS#1011)\n      :     :     +- *FileScan csv [POS#1011] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#997]\n      :           +- *Filter isnotnull(POS#997)\n      :              +- *FileScan csv [POS#997] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#1080], [POS#1066], Inner, BuildRight\n      :     :- *Project [POS#1080]\n      :     :  +- *Filter isnotnull(POS#1080)\n      :     :     +- *FileScan csv [POS#1080] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#1066]\n      :           +- *Filter isnotnull(POS#1066)\n      :              +- *FileScan csv [POS#1066] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#1149], [POS#1135], Inner, BuildRight\n      :     :- *Project [POS#1149]\n      :     :  +- *Filter isnotnull(POS#1149)\n      :     :     +- *FileScan csv [POS#1149] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#1135]\n      :           +- *Filter isnotnull(POS#1135)\n      :              +- *FileScan csv [POS#1135] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#1218], [POS#1204], Inner, BuildRight\n      :     :- *Project [POS#1218]\n      :     :  +- *Filter isnotnull(POS#1218)\n      :     :     +- *FileScan csv [POS#1218] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#1204]\n      :           +- *Filter isnotnull(POS#1204)\n      :              +- *FileScan csv [POS#1204] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#1287], [POS#1273], Inner, BuildRight\n      :     :- *Project [POS#1287]\n      :     :  +- *Filter isnotnull(POS#1287)\n      :     :     +- *FileScan csv [POS#1287] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#1273]\n      :           +- *Filter isnotnull(POS#1273)\n      :              +- *FileScan csv [POS#1273] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#1356], [POS#1342], Inner, BuildRight\n      :     :- *Project [POS#1356]\n      :     :  +- *Filter isnotnull(POS#1356)\n      :     :     +- *FileScan csv [POS#1356] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#1342]\n      :           +- *Filter isnotnull(POS#1342)\n      :              +- *FileScan csv [POS#1342] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#1425], [POS#1411], Inner, BuildRight\n      :     :- *Project [POS#1425]\n      :     :  +- *Filter isnotnull(POS#1425)\n      :     :     +- *FileScan csv [POS#1425] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#1411]\n      :           +- *Filter isnotnull(POS#1411)\n      :              +- *FileScan csv [POS#1411] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      +- *Project\n         +- *BroadcastHashJoin [POS#1494], [POS#1480], Inner, BuildRight\n            :- *Project [POS#1494]\n            :  +- *Filter isnotnull(POS#1494)\n            :     +- *FileScan csv [POS#1494] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n            +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n               +- *Project [POS#1480]\n                  +- *Filter isnotnull(POS#1480)\n                     +- *FileScan csv [POS#1480] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange.doExecute(ShuffleExchange.scala:115)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:228)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:275)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2430)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2429)\n\tat org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:2837)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:2836)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2429)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.util.concurrent.TimeoutException: Futures timed out after [300 seconds]\n\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)\n\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)\n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:201)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:123)\n\tat org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:248)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:126)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:98)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:197)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:82)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:155)\n\tat org.apache.spark.sql.execution.ProjectExec.consume(basicPhysicalOperators.scala:36)\n\tat org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:68)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:155)\n\tat org.apache.spark.sql.execution.FilterExec.consume(basicPhysicalOperators.scala:88)\n\tat org.apache.spark.sql.execution.FilterExec.doConsume(basicPhysicalOperators.scala:209)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:155)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.consume(DataSourceScanExec.scala:155)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doProduce(DataSourceScanExec.scala:361)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:80)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:80)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.produce(DataSourceScanExec.scala:155)\n\tat org.apache.spark.sql.execution.FilterExec.doProduce(basicPhysicalOperators.scala:128)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:80)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:80)\n\tat org.apache.spark.sql.execution.FilterExec.produce(basicPhysicalOperators.scala:88)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:46)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:80)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:80)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:36)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:77)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:80)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:80)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:38)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:46)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:80)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:80)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:36)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:331)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:372)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.UnionExec$$anonfun$doExecute$1.apply(basicPhysicalOperators.scala:556)\n\tat org.apache.spark.sql.execution.UnionExec$$anonfun$doExecute$1.apply(basicPhysicalOperators.scala:556)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.execution.UnionExec.doExecute(basicPhysicalOperators.scala:556)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange.prepareShuffleDependency(ShuffleExchange.scala:88)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:124)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:115)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 35 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-002f473d8273>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Count of rows: {0}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreqDF_working\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfreqDF_working\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/leehongkai/Spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \"\"\"\n\u001b[0;32m--> 427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/leehongkai/Spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/leehongkai/Spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/leehongkai/Spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1681.count.\n: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange SinglePartition\n+- *HashAggregate(keys=[], functions=[partial_count(1)], output=[count#3210L])\n   +- Union\n      :- *Project\n      :  +- Scan ExistingRDD[CHROM#15,POS#16,N_ALLELES#17,N_CHR#18,ALLELE_FREQ_1#19,ALLELE_FREQ_2#20,ID#21]\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#1563], [POS#1549], Inner, BuildRight\n      :     :- *Project [POS#1563]\n      :     :  +- *Filter isnotnull(POS#1563)\n      :     :     +- *FileScan csv [POS#1563] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#1549]\n      :           +- *Filter isnotnull(POS#1549)\n      :              +- *FileScan csv [POS#1549] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#1632], [POS#1618], Inner, BuildRight\n      :     :- *Project [POS#1632]\n      :     :  +- *Filter isnotnull(POS#1632)\n      :     :     +- *FileScan csv [POS#1632] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#1618]\n      :           +- *Filter isnotnull(POS#1618)\n      :              +- *FileScan csv [POS#1618] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#1701], [POS#1687], Inner, BuildRight\n      :     :- *Project [POS#1701]\n      :     :  +- *Filter isnotnull(POS#1701)\n      :     :     +- *FileScan csv [POS#1701] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#1687]\n      :           +- *Filter isnotnull(POS#1687)\n      :              +- *FileScan csv [POS#1687] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#1770], [POS#1756], Inner, BuildRight\n      :     :- *Project [POS#1770]\n      :     :  +- *Filter isnotnull(POS#1770)\n      :     :     +- *FileScan csv [POS#1770] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#1756]\n      :           +- *Filter isnotnull(POS#1756)\n      :              +- *FileScan csv [POS#1756] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#1839], [POS#1825], Inner, BuildRight\n      :     :- *Project [POS#1839]\n      :     :  +- *Filter isnotnull(POS#1839)\n      :     :     +- *FileScan csv [POS#1839] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#1825]\n      :           +- *Filter isnotnull(POS#1825)\n      :              +- *FileScan csv [POS#1825] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#1908], [POS#1894], Inner, BuildRight\n      :     :- *Project [POS#1908]\n      :     :  +- *Filter isnotnull(POS#1908)\n      :     :     +- *FileScan csv [POS#1908] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#1894]\n      :           +- *Filter isnotnull(POS#1894)\n      :              +- *FileScan csv [POS#1894] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#1977], [POS#1963], Inner, BuildRight\n      :     :- *Project [POS#1977]\n      :     :  +- *Filter isnotnull(POS#1977)\n      :     :     +- *FileScan csv [POS#1977] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#1963]\n      :           +- *Filter isnotnull(POS#1963)\n      :              +- *FileScan csv [POS#1963] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#2046], [POS#2032], Inner, BuildRight\n      :     :- *Project [POS#2046]\n      :     :  +- *Filter isnotnull(POS#2046)\n      :     :     +- *FileScan csv [POS#2046] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#2032]\n      :           +- *Filter isnotnull(POS#2032)\n      :              +- *FileScan csv [POS#2032] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#2115], [POS#2101], Inner, BuildRight\n      :     :- *Project [POS#2115]\n      :     :  +- *Filter isnotnull(POS#2115)\n      :     :     +- *FileScan csv [POS#2115] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#2101]\n      :           +- *Filter isnotnull(POS#2101)\n      :              +- *FileScan csv [POS#2101] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#2184], [POS#2170], Inner, BuildRight\n      :     :- *Project [POS#2184]\n      :     :  +- *Filter isnotnull(POS#2184)\n      :     :     +- *FileScan csv [POS#2184] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#2170]\n      :           +- *Filter isnotnull(POS#2170)\n      :              +- *FileScan csv [POS#2170] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#2253], [POS#2239], Inner, BuildRight\n      :     :- *Project [POS#2253]\n      :     :  +- *Filter isnotnull(POS#2253)\n      :     :     +- *FileScan csv [POS#2253] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#2239]\n      :           +- *Filter isnotnull(POS#2239)\n      :              +- *FileScan csv [POS#2239] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#2322], [POS#2308], Inner, BuildRight\n      :     :- *Project [POS#2322]\n      :     :  +- *Filter isnotnull(POS#2322)\n      :     :     +- *FileScan csv [POS#2322] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#2308]\n      :           +- *Filter isnotnull(POS#2308)\n      :              +- *FileScan csv [POS#2308] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#2391], [POS#2377], Inner, BuildRight\n      :     :- *Project [POS#2391]\n      :     :  +- *Filter isnotnull(POS#2391)\n      :     :     +- *FileScan csv [POS#2391] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#2377]\n      :           +- *Filter isnotnull(POS#2377)\n      :              +- *FileScan csv [POS#2377] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#2460], [POS#2446], Inner, BuildRight\n      :     :- *Project [POS#2460]\n      :     :  +- *Filter isnotnull(POS#2460)\n      :     :     +- *FileScan csv [POS#2460] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#2446]\n      :           +- *Filter isnotnull(POS#2446)\n      :              +- *FileScan csv [POS#2446] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#2529], [POS#2515], Inner, BuildRight\n      :     :- *Project [POS#2529]\n      :     :  +- *Filter isnotnull(POS#2529)\n      :     :     +- *FileScan csv [POS#2529] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#2515]\n      :           +- *Filter isnotnull(POS#2515)\n      :              +- *FileScan csv [POS#2515] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#2598], [POS#2584], Inner, BuildRight\n      :     :- *Project [POS#2598]\n      :     :  +- *Filter isnotnull(POS#2598)\n      :     :     +- *FileScan csv [POS#2598] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#2584]\n      :           +- *Filter isnotnull(POS#2584)\n      :              +- *FileScan csv [POS#2584] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#2667], [POS#2653], Inner, BuildRight\n      :     :- *Project [POS#2667]\n      :     :  +- *Filter isnotnull(POS#2667)\n      :     :     +- *FileScan csv [POS#2667] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#2653]\n      :           +- *Filter isnotnull(POS#2653)\n      :              +- *FileScan csv [POS#2653] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#2736], [POS#2722], Inner, BuildRight\n      :     :- *Project [POS#2736]\n      :     :  +- *Filter isnotnull(POS#2736)\n      :     :     +- *FileScan csv [POS#2736] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#2722]\n      :           +- *Filter isnotnull(POS#2722)\n      :              +- *FileScan csv [POS#2722] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#2805], [POS#2791], Inner, BuildRight\n      :     :- *Project [POS#2805]\n      :     :  +- *Filter isnotnull(POS#2805)\n      :     :     +- *FileScan csv [POS#2805] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#2791]\n      :           +- *Filter isnotnull(POS#2791)\n      :              +- *FileScan csv [POS#2791] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#2874], [POS#2860], Inner, BuildRight\n      :     :- *Project [POS#2874]\n      :     :  +- *Filter isnotnull(POS#2874)\n      :     :     +- *FileScan csv [POS#2874] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#2860]\n      :           +- *Filter isnotnull(POS#2860)\n      :              +- *FileScan csv [POS#2860] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#2943], [POS#2929], Inner, BuildRight\n      :     :- *Project [POS#2943]\n      :     :  +- *Filter isnotnull(POS#2943)\n      :     :     +- *FileScan csv [POS#2943] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#2929]\n      :           +- *Filter isnotnull(POS#2929)\n      :              +- *FileScan csv [POS#2929] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#3012], [POS#2998], Inner, BuildRight\n      :     :- *Project [POS#3012]\n      :     :  +- *Filter isnotnull(POS#3012)\n      :     :     +- *FileScan csv [POS#3012] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#2998]\n      :           +- *Filter isnotnull(POS#2998)\n      :              +- *FileScan csv [POS#2998] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgMalay_vcf/2012_05/snp..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- Scan ExistingRDD[CHROM#0,POS#1,N_ALLELES#2,N_CHR#3,ALLELE_FREQ_1#4,ALLELE_FREQ_2#5,ID#6]\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#45], [POS#31], Inner, BuildRight\n      :     :- *Project [POS#45]\n      :     :  +- *Filter isnotnull(POS#45)\n      :     :     +- *FileScan csv [POS#45] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#31]\n      :           +- *Filter isnotnull(POS#31)\n      :              +- *FileScan csv [POS#31] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#114], [POS#100], Inner, BuildRight\n      :     :- *Project [POS#114]\n      :     :  +- *Filter isnotnull(POS#114)\n      :     :     +- *FileScan csv [POS#114] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#100]\n      :           +- *Filter isnotnull(POS#100)\n      :              +- *FileScan csv [POS#100] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#183], [POS#169], Inner, BuildRight\n      :     :- *Project [POS#183]\n      :     :  +- *Filter isnotnull(POS#183)\n      :     :     +- *FileScan csv [POS#183] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#169]\n      :           +- *Filter isnotnull(POS#169)\n      :              +- *FileScan csv [POS#169] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#252], [POS#238], Inner, BuildRight\n      :     :- *Project [POS#252]\n      :     :  +- *Filter isnotnull(POS#252)\n      :     :     +- *FileScan csv [POS#252] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#238]\n      :           +- *Filter isnotnull(POS#238)\n      :              +- *FileScan csv [POS#238] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#321], [POS#307], Inner, BuildRight\n      :     :- *Project [POS#321]\n      :     :  +- *Filter isnotnull(POS#321)\n      :     :     +- *FileScan csv [POS#321] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#307]\n      :           +- *Filter isnotnull(POS#307)\n      :              +- *FileScan csv [POS#307] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#390], [POS#376], Inner, BuildRight\n      :     :- *Project [POS#390]\n      :     :  +- *Filter isnotnull(POS#390)\n      :     :     +- *FileScan csv [POS#390] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#376]\n      :           +- *Filter isnotnull(POS#376)\n      :              +- *FileScan csv [POS#376] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#459], [POS#445], Inner, BuildRight\n      :     :- *Project [POS#459]\n      :     :  +- *Filter isnotnull(POS#459)\n      :     :     +- *FileScan csv [POS#459] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#445]\n      :           +- *Filter isnotnull(POS#445)\n      :              +- *FileScan csv [POS#445] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#528], [POS#514], Inner, BuildRight\n      :     :- *Project [POS#528]\n      :     :  +- *Filter isnotnull(POS#528)\n      :     :     +- *FileScan csv [POS#528] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#514]\n      :           +- *Filter isnotnull(POS#514)\n      :              +- *FileScan csv [POS#514] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#597], [POS#583], Inner, BuildRight\n      :     :- *Project [POS#597]\n      :     :  +- *Filter isnotnull(POS#597)\n      :     :     +- *FileScan csv [POS#597] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#583]\n      :           +- *Filter isnotnull(POS#583)\n      :              +- *FileScan csv [POS#583] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#666], [POS#652], Inner, BuildRight\n      :     :- *Project [POS#666]\n      :     :  +- *Filter isnotnull(POS#666)\n      :     :     +- *FileScan csv [POS#666] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#652]\n      :           +- *Filter isnotnull(POS#652)\n      :              +- *FileScan csv [POS#652] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#735], [POS#721], Inner, BuildRight\n      :     :- *Project [POS#735]\n      :     :  +- *Filter isnotnull(POS#735)\n      :     :     +- *FileScan csv [POS#735] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#721]\n      :           +- *Filter isnotnull(POS#721)\n      :              +- *FileScan csv [POS#721] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#804], [POS#790], Inner, BuildRight\n      :     :- *Project [POS#804]\n      :     :  +- *Filter isnotnull(POS#804)\n      :     :     +- *FileScan csv [POS#804] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#790]\n      :           +- *Filter isnotnull(POS#790)\n      :              +- *FileScan csv [POS#790] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#873], [POS#859], Inner, BuildRight\n      :     :- *Project [POS#873]\n      :     :  +- *Filter isnotnull(POS#873)\n      :     :     +- *FileScan csv [POS#873] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#859]\n      :           +- *Filter isnotnull(POS#859)\n      :              +- *FileScan csv [POS#859] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#942], [POS#928], Inner, BuildRight\n      :     :- *Project [POS#942]\n      :     :  +- *Filter isnotnull(POS#942)\n      :     :     +- *FileScan csv [POS#942] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#928]\n      :           +- *Filter isnotnull(POS#928)\n      :              +- *FileScan csv [POS#928] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#1011], [POS#997], Inner, BuildRight\n      :     :- *Project [POS#1011]\n      :     :  +- *Filter isnotnull(POS#1011)\n      :     :     +- *FileScan csv [POS#1011] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#997]\n      :           +- *Filter isnotnull(POS#997)\n      :              +- *FileScan csv [POS#997] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#1080], [POS#1066], Inner, BuildRight\n      :     :- *Project [POS#1080]\n      :     :  +- *Filter isnotnull(POS#1080)\n      :     :     +- *FileScan csv [POS#1080] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#1066]\n      :           +- *Filter isnotnull(POS#1066)\n      :              +- *FileScan csv [POS#1066] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#1149], [POS#1135], Inner, BuildRight\n      :     :- *Project [POS#1149]\n      :     :  +- *Filter isnotnull(POS#1149)\n      :     :     +- *FileScan csv [POS#1149] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#1135]\n      :           +- *Filter isnotnull(POS#1135)\n      :              +- *FileScan csv [POS#1135] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#1218], [POS#1204], Inner, BuildRight\n      :     :- *Project [POS#1218]\n      :     :  +- *Filter isnotnull(POS#1218)\n      :     :     +- *FileScan csv [POS#1218] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#1204]\n      :           +- *Filter isnotnull(POS#1204)\n      :              +- *FileScan csv [POS#1204] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#1287], [POS#1273], Inner, BuildRight\n      :     :- *Project [POS#1287]\n      :     :  +- *Filter isnotnull(POS#1287)\n      :     :     +- *FileScan csv [POS#1287] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#1273]\n      :           +- *Filter isnotnull(POS#1273)\n      :              +- *FileScan csv [POS#1273] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#1356], [POS#1342], Inner, BuildRight\n      :     :- *Project [POS#1356]\n      :     :  +- *Filter isnotnull(POS#1356)\n      :     :     +- *FileScan csv [POS#1356] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#1342]\n      :           +- *Filter isnotnull(POS#1342)\n      :              +- *FileScan csv [POS#1342] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :- *Project\n      :  +- *BroadcastHashJoin [POS#1425], [POS#1411], Inner, BuildRight\n      :     :- *Project [POS#1425]\n      :     :  +- *Filter isnotnull(POS#1425)\n      :     :     +- *FileScan csv [POS#1425] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n      :        +- *Project [POS#1411]\n      :           +- *Filter isnotnull(POS#1411)\n      :              +- *FileScan csv [POS#1411] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n      +- *Project\n         +- *BroadcastHashJoin [POS#1494], [POS#1480], Inner, BuildRight\n            :- *Project [POS#1494]\n            :  +- *Filter isnotnull(POS#1494)\n            :     +- *FileScan csv [POS#1494] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n            +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n               +- *Project [POS#1480]\n                  +- *Filter isnotnull(POS#1480)\n                     +- *FileScan csv [POS#1480] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Volumes/Samsung_T3/SgPopulationGenetics/GenoSpark/SgIndian_vcf/dataFreeze..., PartitionFilters: [], PushedFilters: [IsNotNull(POS)], ReadSchema: struct<POS:int>\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange.doExecute(ShuffleExchange.scala:115)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:228)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:275)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2430)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2429)\n\tat org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:2837)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:2836)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2429)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.util.concurrent.TimeoutException: Futures timed out after [300 seconds]\n\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)\n\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)\n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:201)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:123)\n\tat org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:248)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:126)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:98)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:197)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:82)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:155)\n\tat org.apache.spark.sql.execution.ProjectExec.consume(basicPhysicalOperators.scala:36)\n\tat org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:68)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:155)\n\tat org.apache.spark.sql.execution.FilterExec.consume(basicPhysicalOperators.scala:88)\n\tat org.apache.spark.sql.execution.FilterExec.doConsume(basicPhysicalOperators.scala:209)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:155)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.consume(DataSourceScanExec.scala:155)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doProduce(DataSourceScanExec.scala:361)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:80)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:80)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.produce(DataSourceScanExec.scala:155)\n\tat org.apache.spark.sql.execution.FilterExec.doProduce(basicPhysicalOperators.scala:128)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:80)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:80)\n\tat org.apache.spark.sql.execution.FilterExec.produce(basicPhysicalOperators.scala:88)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:46)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:80)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:80)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:36)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:77)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:80)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:80)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:38)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:46)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:80)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:80)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:36)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:331)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:372)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.UnionExec$$anonfun$doExecute$1.apply(basicPhysicalOperators.scala:556)\n\tat org.apache.spark.sql.execution.UnionExec$$anonfun$doExecute$1.apply(basicPhysicalOperators.scala:556)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.execution.UnionExec.doExecute(basicPhysicalOperators.scala:556)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange.prepareShuffleDependency(ShuffleExchange.scala:88)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:124)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:115)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 35 more\n"
     ]
    }
   ],
   "source": [
    "print('Count of rows: {0}'.format(freqDF_working.count()))\n",
    "freqDF_working.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freqDF_working.select(freqDF_working.ID, freqDF_working.ALLELE_FREQ_1, freqDF_working.ALLELE_FREQ_2).filter(freqDF_working.ID == \"rs4109479\").show()\n",
    "freqDF_working.select(\"ID\", \"CHROM\", \"ALLELE_FREQ_1\", \"ALLELE_FREQ_2\").filter(\"ID == 'rs4109479'\").show()\n",
    "spark.sql(\"select ID, ALLELE_FREQ_1, ALLELE_FREQ_2 from freqDF_working where ID = 'rs4109479'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print('Count of rows: {0}'.format(freqDF_working.count()))\n",
    "# print('Count of distinct rows: {0}'.format(freqDF_working.distinct().count()))\n",
    "# freqDF_working = freqDF_working.dropDuplicates()\n",
    "# print('Count of rows: {0}'.format(freqDF_working.count()))\n",
    "# print('Count of distinct rows: {0}'.format(freqDF_working.distinct().count()))\n",
    "# print('Count of IDs: {0}'.format(freqDF_working.count()))\n",
    "# print('Count of distinct IDs: {0}'.format(freqDF_working.select(freqDF_working.ID).distinct().count()))\n",
    "# freqDF_working.where(\"ID = '.'\").show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
